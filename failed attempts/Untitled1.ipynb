{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XG-RXd1uEAf",
        "outputId": "8f407318-2194-4cd1-cdee-a070e48d2a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow hmmlearn numpy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from hmmlearn import hmm\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict, deque\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# ---\n",
        "# 1. Part 0: Corpus Loading and Preprocessing\n",
        "# ---\n",
        "def load_corpus(filename=\"corpus.txt\"):\n",
        "    \"\"\"\n",
        "    Loads the 50,000-word corpus.\n",
        "    The provided file is a single text block.\n",
        "    \"\"\"\n",
        "    print(f\"Loading corpus from {filename}...\")\n",
        "    try:\n",
        "        # Use the content from the provided 'corpus.txt'\n",
        "        with open(filename, 'r') as f:\n",
        "            full_content = f.read()\n",
        "\n",
        "        # Split by any whitespace and filter for valid words\n",
        "        all_words = re.split(r'\\s+', full_content)\n",
        "        valid_words = sorted(list(set(\n",
        "            word.lower() for word in all_words if word.isalpha()\n",
        "        )))\n",
        "\n",
        "        print(f\"Loaded {len(valid_words)} unique, valid words.\")\n",
        "        return valid_words\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filename} not found.\")\n",
        "        print(\"Please ensure 'corpus.txt' is in the same directory.\")\n",
        "        return []\n",
        "\n",
        "def group_words_by_length(words):\n",
        "    \"\"\"\n",
        "    Groups words by their length, as hinted for HMM training.\n",
        "    \"\"\"\n",
        "    words_by_length = defaultdict(list)\n",
        "    for word in words:\n",
        "        words_by_length[len(word)].append(word)\n",
        "    print(f\"Grouped words into {len(words_by_length)} length-based buckets.\")\n",
        "    return words_by_length\n",
        "\n",
        "def word_to_sequence(word):\n",
        "    \"\"\"Converts a word into a numpy array of integer emissions (0-25).\"\"\"\n",
        "    return np.array([ord(char) - ord('a') for char in word]).reshape(-1, 1)\n",
        "\n",
        "def sequence_to_word(seq):\n",
        "    \"\"\"Converts a sequence of integers back to a word.\"\"\"\n",
        "    return \"\".join([chr(ord('a') + num) for num in seq.flatten()])\n",
        "\n",
        "# ---\n",
        "# 2. Part 1: The Hidden Markov Model (HMM) Oracle\n",
        "# ---\n",
        "class HMMOracle:\n",
        "    \"\"\"\n",
        "    Implements the hybrid probabilistic oracle.\n",
        "    It uses a word-list filter as the primary source and a\n",
        "    positional HMM as a fallback.\n",
        "    \"\"\"\n",
        "    def __init__(self, words_by_length):\n",
        "        self.words_by_length = words_by_length\n",
        "        self.max_len = max(words_by_length.keys())\n",
        "        self.hmms = {} # Stores HMMs by length\n",
        "        self.pattern_cache = {}\n",
        "\n",
        "    def _get_pattern(self, masked_word, guessed_letters):\n",
        "        \"\"\"Creates a regex pattern to filter the word list.\"\"\"\n",
        "        key = (masked_word, tuple(sorted(guessed_letters)))\n",
        "        if key in self.pattern_cache:\n",
        "            return self.pattern_cache[key]\n",
        "\n",
        "        pattern = list(masked_word)\n",
        "        wrong_guesses = guessed_letters - set(masked_word)\n",
        "\n",
        "        regex_parts = []\n",
        "        for char in pattern:\n",
        "            if char == '_':\n",
        "                negation_set = \"\".join(sorted(wrong_guesses))\n",
        "                # Fix: Handle empty negation set\n",
        "                if negation_set:\n",
        "                    regex_parts.append(f\"[^{negation_set}]\")\n",
        "                else:\n",
        "                    regex_parts.append(\".\") # Any character if no wrong guesses\n",
        "            else:\n",
        "                regex_parts.append(char)\n",
        "\n",
        "        regex_str = f\"^{''.join(regex_parts)}$\"\n",
        "        self.pattern_cache[key] = re.compile(regex_str)\n",
        "        return self.pattern_cache[key]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains one HMM for each word length.\n",
        "        State = Position in word (Hidden State)\n",
        "        Emission = Letter (Emission)\n",
        "        \"\"\"\n",
        "        print(\"Training HMMs (positional frequency models)...\")\n",
        "        for length, word_list in self.words_by_length.items():\n",
        "            if not word_list:\n",
        "                continue\n",
        "\n",
        "            model = hmm.CategoricalHMM(\n",
        "                n_components=length, n_features=26,\n",
        "                init_params=\"\", params=\"\"\n",
        "            )\n",
        "            model.startprob_ = np.array([1.0] + [0.0] * (length - 1))\n",
        "            transmat = np.zeros((length, length))\n",
        "            for i in range(length - 1):\n",
        "                transmat[i, i + 1] = 1.0\n",
        "            transmat[length - 1, length - 1] = 1.0\n",
        "            model.transmat_ = transmat\n",
        "\n",
        "            emission_prob = np.ones((length, 26))\n",
        "            for word in word_list:\n",
        "                for i, char in enumerate(word):\n",
        "                    emission_prob[i, ord(char) - ord('a')] += 1\n",
        "\n",
        "            emission_prob /= np.sum(emission_prob, axis=1, keepdims=True)\n",
        "            model.emissionprob_ = emission_prob\n",
        "            self.hmms[length] = model\n",
        "        print(\"HMM training complete.\")\n",
        "\n",
        "    def get_probabilities(self, masked_word, guessed_letters):\n",
        "        \"\"\"Calculates the probability distribution over the alphabet.\"\"\"\n",
        "        length = len(masked_word)\n",
        "        if length not in self.words_by_length:\n",
        "            return np.zeros(26)\n",
        "\n",
        "        pattern = self._get_pattern(masked_word, guessed_letters)\n",
        "        relevant_words = [\n",
        "            word for word in self.words_by_length[length] if pattern.match(word)\n",
        "        ]\n",
        "\n",
        "        probs = np.zeros(26)\n",
        "        available_letters = set(string.ascii_lowercase) - guessed_letters\n",
        "\n",
        "        if relevant_words:\n",
        "            for word in relevant_words:\n",
        "                for i, char in enumerate(word):\n",
        "                    if masked_word[i] == '_':\n",
        "                        probs[ord(char) - ord('a')] += 1\n",
        "\n",
        "            if np.sum(probs) > 0:\n",
        "                probs /= np.sum(probs)\n",
        "\n",
        "            for i in range(26):\n",
        "                if chr(ord('a') + i) not in available_letters:\n",
        "                    probs[i] = 0\n",
        "            return probs\n",
        "\n",
        "        model = self.hmms.get(length)\n",
        "        if model is None:\n",
        "            return np.zeros(26)\n",
        "\n",
        "        probs = np.zeros(26)\n",
        "        for i, char in enumerate(masked_word):\n",
        "            if char == '_':\n",
        "                probs += model.emissionprob_[i, :]\n",
        "\n",
        "        for i in range(26):\n",
        "            if chr(ord('a') + i) not in available_letters:\n",
        "                probs[i] = 0\n",
        "\n",
        "        total_prob = np.sum(probs)\n",
        "        if total_prob > 0:\n",
        "            return probs / total_prob\n",
        "        else:\n",
        "            return np.zeros(26)\n",
        "\n",
        "# ---\n",
        "# 3. Part 2: The Reinforcement Learning (RL) Environment (STABLE REWARDS)\n",
        "# ---\n",
        "class HangmanEnv:\n",
        "    \"\"\"Implements the Hangman Game Environment for the RL agent.\"\"\"\n",
        "    def __init__(self, word_list, oracle, max_lives=6):\n",
        "        self.all_words = word_list\n",
        "        self.oracle = oracle\n",
        "        self.max_lives = max_lives\n",
        "        self._reset_game_stats()\n",
        "\n",
        "    def _reset_game_stats(self):\n",
        "        \"\"\"Generates a new game state.\"\"\"\n",
        "        self.word = random.choice(self.all_words)\n",
        "        self.word_len = len(self.word)\n",
        "        self.masked_word = \"_\" * self.word_len\n",
        "        self.lives_left = self.max_lives\n",
        "        self.guessed_letters = set()\n",
        "        self.game_wrong_guesses = 0\n",
        "        self.game_repeated_guesses = 0\n",
        "        self.game_won = False\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Defines the state representation for the RL agent.\"\"\"\n",
        "        hmm_probs = self.oracle.get_probabilities(\n",
        "            self.masked_word, self.guessed_letters\n",
        "        )\n",
        "        guessed_vec = np.array([\n",
        "            1.0 if c in self.guessed_letters else 0.0\n",
        "            for c in string.ascii_lowercase\n",
        "        ])\n",
        "        lives_norm = self.lives_left / self.max_lives\n",
        "        blanks_norm = self.masked_word.count('_') / self.word_len\n",
        "        state = np.concatenate([\n",
        "            hmm_probs, guessed_vec, [lives_norm], [blanks_norm]\n",
        "        ])\n",
        "        return state.astype(np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment and returns the initial state.\"\"\"\n",
        "        self._reset_game_stats()\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Executes one action (guessing a letter) in the environment.\"\"\"\n",
        "        letter = chr(ord('a') + action)\n",
        "        done = False\n",
        "        info = {\"repeated\": 0, \"wrong\": 0, \"won\": False, \"word\": self.word, \"masked\": self.masked_word}\n",
        "\n",
        "        if letter in self.guessed_letters:\n",
        "            self.game_repeated_guesses += 1\n",
        "            info[\"repeated\"] = 1\n",
        "            reward = -2\n",
        "            return self._get_state(), reward, done, info\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "\n",
        "        if letter in self.word:\n",
        "            new_masked_word = list(self.masked_word)\n",
        "            for i, char in enumerate(self.word):\n",
        "                if char == letter:\n",
        "                    new_masked_word[i] = letter\n",
        "            self.masked_word = \"\".join(new_masked_word)\n",
        "            reward = 10  # More aggressive positive reward\n",
        "            info[\"wrong\"] = 0\n",
        "        else:\n",
        "            self.lives_left -= 1\n",
        "            info[\"wrong\"] = 1\n",
        "            self.game_wrong_guesses += 1\n",
        "            reward = -5 # Penalty from formula\n",
        "\n",
        "        if \"_\" not in self.masked_word:\n",
        "            done = True\n",
        "            self.game_won = True\n",
        "            info[\"won\"] = True\n",
        "            reward = 100 # Large, stable win bonus\n",
        "        elif self.lives_left == 0:\n",
        "            done = True\n",
        "            self.game_won = False\n",
        "            info[\"won\"] = False\n",
        "            reward = -100 # Large, stable loss penalty\n",
        "\n",
        "        return self._get_state(), reward, done, info\n",
        "\n",
        "# ---\n",
        "# 4. Part 2: The Reinforcement Learning (RL) Agent (TUNED HYPERPARAMETERS)\n",
        "# ---\n",
        "class DQNAgent:\n",
        "    \"\"\"Implements a Deep Q-Network (DQN) agent.\"\"\"\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.0001, gamma=0.99):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size # Back to 26\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.gamma = gamma    # Discount factor (high for long-term planning)\n",
        "        self.learning_rate = learning_rate # Lowered for stability\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9992 # Fast decay\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Builds the neural network to approximate the Q-function.\"\"\"\n",
        "        model = Sequential([\n",
        "            Input(shape=(self.state_size,)),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear') # Output 26 actions\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"Syncs the Target Network with the weights of the Main Network.\"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Stores an experience tuple in the replay buffer.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Chooses an action (0-25) using an epsilon-greedy policy.\"\"\"\n",
        "        guessed_mask = state[0][26:52].astype(bool)\n",
        "        available_actions = [i for i, guessed in enumerate(guessed_mask) if not guessed]\n",
        "        if not available_actions:\n",
        "            return 0\n",
        "\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "\n",
        "        act_values = self.model.predict(state, verbose=0)[0]\n",
        "        masked_act_values = [\n",
        "            act_values[i] if i in available_actions else -np.inf\n",
        "            for i in range(self.action_size)\n",
        "        ]\n",
        "        return np.argmax(masked_act_values)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        \"\"\"Trains the model on a minibatch from the replay buffer.\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = np.vstack([t[0] for t in minibatch])\n",
        "        next_states = np.vstack([t[3] for t in minibatch])\n",
        "\n",
        "        q_values_current = self.model.predict(states, verbose=0)\n",
        "        q_values_next_target = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        targets = []\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + self.gamma * np.amax(q_values_next_target[i])\n",
        "\n",
        "            current_q_target = q_values_current[i]\n",
        "            current_q_target[action] = target\n",
        "            targets.append(current_q_target)\n",
        "\n",
        "        self.model.fit(states, np.array(targets), epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "# 5. Global Setup and Training Loop\n",
        "# ---\n",
        "\n",
        "# --- 1. Load and Prepare Data ---\n",
        "print(\"--- Loading and Preparing Data ---\")\n",
        "all_words = load_corpus(\"corpus.txt\")\n",
        "words_by_length = group_words_by_length(all_words)\n",
        "\n",
        "# --- 2. Build HMM Oracle ---\n",
        "print(\"\\n--- Building HMM Oracle ---\")\n",
        "oracle = HMMOracle(words_by_length)\n",
        "oracle.train()\n",
        "\n",
        "# --- 3. Build Environment and Agent (GLOBAL) ---\n",
        "print(\"\\n--- Initializing Agent and Environment ---\")\n",
        "env = HangmanEnv(all_words, oracle, max_lives=6)\n",
        "STATE_SIZE = 54\n",
        "ACTION_SIZE = 26 # Back to 26\n",
        "\n",
        "# Create the agent in the global scope\n",
        "agent = DQNAgent(STATE_SIZE, ACTION_SIZE)\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "EPISODES = 6500  # Run just long enough for epsilon to decay\n",
        "BATCH_SIZE = 64\n",
        "UPDATE_TARGET_FREQ = 500 # Slower target update for stability\n",
        "\n",
        "print(f\"\\n--- Starting RL Agent Training ({EPISODES} episodes) ---\")\n",
        "start_time = time.time()\n",
        "episode_rewards = []\n",
        "episode_wins = [] # Track win rate\n",
        "\n",
        "for e in range(1, EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, STATE_SIZE])\n",
        "    done = False\n",
        "    total_episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            episode_rewards.append(total_episode_reward)\n",
        "            if info[\"won\"]:\n",
        "                episode_wins.append(1)\n",
        "            else:\n",
        "                episode_wins.append(0)\n",
        "\n",
        "    agent.replay(BATCH_SIZE)\n",
        "\n",
        "    if e % UPDATE_TARGET_FREQ == 0:\n",
        "        agent.update_target_model()\n",
        "        print(f\"--- Target network updated at episode {e} ---\")\n",
        "\n",
        "    if e % 100 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-100:])\n",
        "        win_rate = np.mean(episode_wins[-100:]) # Also show win rate\n",
        "        print(f\"Episode: {e}/{EPISODES} | \"\n",
        "              f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
        "              f\"Win Rate (last 100): {win_rate*100:.1f}% | \"\n",
        "              f\"Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"--- Training Finished in {training_time:.2f}s ---\")\n",
        "print(\"Agent is trained and ready for evaluation in the next cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf9MJB9huH0s",
        "outputId": "d21de202-aaca-40cf-af1a-b777dbc0b31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Preparing Data ---\n",
            "Loading corpus from corpus.txt...\n",
            "Loaded 49399 unique, valid words.\n",
            "Grouped words into 24 length-based buckets.\n",
            "\n",
            "--- Building HMM Oracle ---\n",
            "Training HMMs (positional frequency models)...\n",
            "HMM training complete.\n",
            "\n",
            "--- Initializing Agent and Environment ---\n",
            "\n",
            "--- Starting RL Agent Training (6500 episodes) ---\n",
            "Episode: 100/6500 | Avg Reward (last 100): -99.60 | Win Rate (last 100): 0.0% | Epsilon: 0.928\n",
            "Episode: 200/6500 | Avg Reward (last 100): -98.10 | Win Rate (last 100): 0.0% | Epsilon: 0.857\n",
            "Episode: 300/6500 | Avg Reward (last 100): -92.15 | Win Rate (last 100): 2.0% | Epsilon: 0.791\n",
            "Episode: 400/6500 | Avg Reward (last 100): -88.90 | Win Rate (last 100): 0.0% | Epsilon: 0.730\n",
            "--- Target network updated at episode 500 ---\n",
            "Episode: 500/6500 | Avg Reward (last 100): -90.50 | Win Rate (last 100): 1.0% | Epsilon: 0.674\n",
            "Episode: 600/6500 | Avg Reward (last 100): -76.85 | Win Rate (last 100): 5.0% | Epsilon: 0.622\n",
            "Episode: 700/6500 | Avg Reward (last 100): -85.95 | Win Rate (last 100): 1.0% | Epsilon: 0.574\n",
            "Episode: 800/6500 | Avg Reward (last 100): -81.10 | Win Rate (last 100): 3.0% | Epsilon: 0.530\n",
            "Episode: 900/6500 | Avg Reward (last 100): -84.85 | Win Rate (last 100): 2.0% | Epsilon: 0.489\n",
            "--- Target network updated at episode 1000 ---\n",
            "Episode: 1000/6500 | Avg Reward (last 100): -79.35 | Win Rate (last 100): 2.0% | Epsilon: 0.452\n",
            "Episode: 1100/6500 | Avg Reward (last 100): -73.30 | Win Rate (last 100): 3.0% | Epsilon: 0.417\n",
            "Episode: 1200/6500 | Avg Reward (last 100): -59.25 | Win Rate (last 100): 9.0% | Epsilon: 0.385\n",
            "Episode: 1300/6500 | Avg Reward (last 100): -71.85 | Win Rate (last 100): 3.0% | Epsilon: 0.355\n",
            "Episode: 1400/6500 | Avg Reward (last 100): -63.15 | Win Rate (last 100): 7.0% | Epsilon: 0.328\n",
            "--- Target network updated at episode 1500 ---\n",
            "Episode: 1500/6500 | Avg Reward (last 100): -71.90 | Win Rate (last 100): 4.0% | Epsilon: 0.303\n",
            "Episode: 1600/6500 | Avg Reward (last 100): -66.70 | Win Rate (last 100): 5.0% | Epsilon: 0.279\n",
            "Episode: 1700/6500 | Avg Reward (last 100): -60.60 | Win Rate (last 100): 8.0% | Epsilon: 0.258\n",
            "Episode: 1800/6500 | Avg Reward (last 100): -67.40 | Win Rate (last 100): 6.0% | Epsilon: 0.238\n",
            "Episode: 1900/6500 | Avg Reward (last 100): -61.55 | Win Rate (last 100): 7.0% | Epsilon: 0.220\n",
            "--- Target network updated at episode 2000 ---\n",
            "Episode: 2000/6500 | Avg Reward (last 100): -57.05 | Win Rate (last 100): 9.0% | Epsilon: 0.203\n",
            "Episode: 2100/6500 | Avg Reward (last 100): -57.45 | Win Rate (last 100): 10.0% | Epsilon: 0.187\n",
            "Episode: 2200/6500 | Avg Reward (last 100): -40.90 | Win Rate (last 100): 15.0% | Epsilon: 0.173\n",
            "Episode: 2300/6500 | Avg Reward (last 100): -45.30 | Win Rate (last 100): 14.0% | Epsilon: 0.160\n",
            "Episode: 2400/6500 | Avg Reward (last 100): -42.15 | Win Rate (last 100): 15.0% | Epsilon: 0.147\n",
            "--- Target network updated at episode 2500 ---\n",
            "Episode: 2500/6500 | Avg Reward (last 100): -52.35 | Win Rate (last 100): 12.0% | Epsilon: 0.136\n",
            "Episode: 2600/6500 | Avg Reward (last 100): -48.45 | Win Rate (last 100): 12.0% | Epsilon: 0.126\n",
            "Episode: 2700/6500 | Avg Reward (last 100): -29.85 | Win Rate (last 100): 19.0% | Epsilon: 0.116\n",
            "Episode: 2800/6500 | Avg Reward (last 100): -45.40 | Win Rate (last 100): 12.0% | Epsilon: 0.107\n",
            "Episode: 2900/6500 | Avg Reward (last 100): -50.10 | Win Rate (last 100): 11.0% | Epsilon: 0.099\n",
            "--- Target network updated at episode 3000 ---\n",
            "Episode: 3000/6500 | Avg Reward (last 100): -56.00 | Win Rate (last 100): 9.0% | Epsilon: 0.091\n",
            "Episode: 3100/6500 | Avg Reward (last 100): -49.65 | Win Rate (last 100): 12.0% | Epsilon: 0.084\n",
            "Episode: 3200/6500 | Avg Reward (last 100): -41.00 | Win Rate (last 100): 14.0% | Epsilon: 0.078\n",
            "Episode: 3300/6500 | Avg Reward (last 100): -41.80 | Win Rate (last 100): 15.0% | Epsilon: 0.072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "# 6. Evaluation Loop\n",
        "# ---\n",
        "\n",
        "EVAL_GAMES = 2000\n",
        "print(f\"\\n--- Starting Evaluation ({EVAL_GAMES} games) ---\")\n",
        "\n",
        "# Set agent to exploitation-only mode\n",
        "agent.epsilon = 0.0\n",
        "\n",
        "total_wins = 0\n",
        "total_wrong_guesses = 0\n",
        "total_repeated_guesses = 0\n",
        "\n",
        "# Load the hidden test set\n",
        "try:\n",
        "    with open(\"test.txt\", 'r') as f:\n",
        "        test_content = f.read()\n",
        "    test_words = re.split(r'\\s+', test_content)\n",
        "    test_words = [word.lower() for word in test_words if word.isalpha()]\n",
        "    # Ensure we only use words from the test set\n",
        "    env.all_words = test_words\n",
        "    print(f\"Loaded {len(test_words)} words from test.txt for evaluation.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: test.txt not found. Evaluating on the training corpus.\")\n",
        "    pass\n",
        "\n",
        "\n",
        "for g in range(1, EVAL_GAMES + 1):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, STATE_SIZE])\n",
        "\n",
        "    done = False\n",
        "    game_wrong = 0\n",
        "    game_repeated = 0\n",
        "\n",
        "    while not done:\n",
        "        # Agent selects best-known action (action masking is internal)\n",
        "        action = agent.act(state)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        state = np.reshape(next_state, [1, STATE_SIZE])\n",
        "\n",
        "        game_wrong += info[\"wrong\"]\n",
        "        game_repeated += info[\"repeated\"]\n",
        "\n",
        "        if done:\n",
        "            if info[\"won\"]:\n",
        "                total_wins += 1\n",
        "            total_wrong_guesses += game_wrong\n",
        "            total_repeated_guesses += game_repeated\n",
        "\n",
        "    if g % 200 == 0:\n",
        "        print(f\"Played game {g}/{EVAL_GAMES}...\")\n",
        "\n",
        "# --- 7. Final Results ---\n",
        "print(\"\\n--- üèÅ Final Evaluation Results ---\")\n",
        "\n",
        "success_rate = total_wins / EVAL_GAMES\n",
        "avg_wrong = total_wrong_guesses / EVAL_GAMES\n",
        "avg_repeated = total_repeated_guesses / EVAL_GAMES\n",
        "\n",
        "# Calculate final score based on the formula\n",
        "final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
        "\n",
        "print(f\"Total Games Played: {EVAL_GAMES}\")\n",
        "print(\"\\n--- Averages ---\")\n",
        "print(f\"Success Rate:         {success_rate * 100:.2f}%\")\n",
        "print(f\"Avg. Wrong Guesses:   {avg_wrong:.3f}\")\n",
        "print(f\"Avg. Repeated Guesses: {avg_repeated:.3f}\")\n",
        "\n",
        "print(\"\\n--- Totals ---\")\n",
        "print(f\"Total Wins:             {total_wins}\")\n",
        "print(f\"Total Wrong Guesses:    {total_wrong_guesses}\")\n",
        "print(f\"Total Repeated Guesses: {total_repeated_guesses}\")\n",
        "\n",
        "print(\"\\n--- SCORE ---\")\n",
        "print(f\"Success Points:  ( {success_rate:.3f} * 2000 )   = {success_rate * 2000:,.0f}\")\n",
        "print(f\"Wrong Penalty:   ( {total_wrong_guesses} * 5 )      = -{total_wrong_guesses * 5:,.0f}\")\n",
        "print(f\"Repeat Penalty:  ( {total_repeated_guesses} * 2 )      = -{total_repeated_guesses * 2:,.0f}\")\n",
        "print(f\"**Final Score**:                            = **{final_score:,.0f}**\")"
      ],
      "metadata": {
        "id": "hQnYVhRKuLeJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}