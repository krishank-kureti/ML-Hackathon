{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**setup and imports**"
      ],
      "metadata": {
        "id": "JxFQuqp2uJK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WXWAWF9psnXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0c74c4-07a0-48fc-9025-16401a27f43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: hmm in /usr/local/lib/python3.12/dist-packages (0.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow hmm numpy hmmlearn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from hmmlearn import hmm\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict, deque\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Preprocessing**\n"
      ],
      "metadata": {
        "id": "FCyWCMbeuSq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus(filename=\"corpus.txt\"):\n",
        "    \"\"\"\n",
        "    Loads the 50,000-word corpus[cite: 18, 37].\n",
        "    The provided file  is a single text block.\n",
        "    \"\"\"\n",
        "    print(f\"Loading corpus from {filename}...\")\n",
        "    try:\n",
        "        # Use the content from the provided 'corpus.txt'\n",
        "        with open(filename, 'r') as f:\n",
        "            full_content = f.read()\n",
        "\n",
        "        # Split by any whitespace and filter for valid words\n",
        "        all_words = re.split(r'\\s+', full_content)\n",
        "        valid_words = sorted(list(set(\n",
        "            word.lower() for word in all_words if word.isalpha()\n",
        "        )))\n",
        "\n",
        "        print(f\"Loaded {len(valid_words)} unique, valid words.\")\n",
        "        return valid_words\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filename} not found.\")\n",
        "        print(\"Please ensure 'corpus.txt' is in the same directory.\")\n",
        "        return []\n",
        "\n",
        "def group_words_by_length(words):\n",
        "    \"\"\"\n",
        "    Groups words by their length, as hinted for HMM training.\n",
        "    \"\"\"\n",
        "    words_by_length = defaultdict(list)\n",
        "    for word in words:\n",
        "        words_by_length[len(word)].append(word)\n",
        "    print(f\"Grouped words into {len(words_by_length)} length-based buckets.\")\n",
        "    return words_by_length\n",
        "\n",
        "def word_to_sequence(word):\n",
        "    \"\"\"Converts a word into a numpy array of integer emissions (0-25).\"\"\"\n",
        "    return np.array([ord(char) - ord('a') for char in word]).reshape(-1, 1)\n",
        "\n",
        "def sequence_to_word(seq):\n",
        "    \"\"\"Converts a sequence of integers back to a word.\"\"\"\n",
        "    return \"\".join([chr(ord('a') + num) for num in seq.flatten()])"
      ],
      "metadata": {
        "id": "AxleohFauVeA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMM**"
      ],
      "metadata": {
        "id": "JuQ9JaHXulU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMMOracle:\n",
        "    \"\"\"\n",
        "    Implements the hybrid probabilistic oracle[cite: 19].\n",
        "    It uses a word-list filter as the primary source and a\n",
        "    positional HMM as a fallback.\n",
        "    \"\"\"\n",
        "    def __init__(self, words_by_length):\n",
        "        self.words_by_length = words_by_length\n",
        "        self.max_len = max(words_by_length.keys())\n",
        "        self.hmms = {} # Stores HMMs by length\n",
        "\n",
        "        # Build regex patterns for word filtering\n",
        "        self.pattern_cache = {}\n",
        "\n",
        "    def _get_pattern(self, masked_word, guessed_letters):\n",
        "        \"\"\"Creates a regex pattern to filter the word list.\"\"\"\n",
        "        key = (masked_word, tuple(sorted(guessed_letters)))\n",
        "        if key in self.pattern_cache:\n",
        "            return self.pattern_cache[key]\n",
        "\n",
        "        pattern = list(masked_word)\n",
        "        available_letters = set(string.ascii_lowercase) - guessed_letters\n",
        "\n",
        "        # Letters that are KNOWN to not be in the word\n",
        "        wrong_guesses = guessed_letters - set(masked_word)\n",
        "\n",
        "        # Regex:\n",
        "        # ^...$ : full word match\n",
        "        # [^abc]: blank spot cannot be a wrong guess\n",
        "        # p: known letter must be 'p'\n",
        "\n",
        "        regex_parts = []\n",
        "        for char in pattern:\n",
        "            if char == '_':\n",
        "                # Blank must be a letter that hasn't been guessed wrong\n",
        "                negation_set = \"\".join(sorted(wrong_guesses))\n",
        "                # Fix: Handle empty negation set\n",
        "                if negation_set:\n",
        "                    regex_parts.append(f\"[^{negation_set}]\")\n",
        "                else:\n",
        "                    regex_parts.append(\".\") # Any character if no wrong guesses\n",
        "            else:\n",
        "                # Must be this exact letter\n",
        "                regex_parts.append(char)\n",
        "\n",
        "        regex_str = f\"^{''.join(regex_parts)}$\"\n",
        "        self.pattern_cache[key] = re.compile(regex_str)\n",
        "        return self.pattern_cache[key]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains one HMM for each word length[cite: 18, 62].\n",
        "        State = Position in word\n",
        "        Emission = Letter\n",
        "        \"\"\"\n",
        "        print(\"Training HMMs (positional frequency models)...\")\n",
        "        for length, word_list in self.words_by_length.items():\n",
        "            if not word_list:\n",
        "                continue\n",
        "\n",
        "            # This is a degenerate HMM where states are positions.\n",
        "            # We fix the transitions to be strictly left-to-right.\n",
        "            model = hmm.CategoricalHMM(\n",
        "                n_components=length, # n_states = word length\n",
        "                n_features=26,       # n_emissions = 'a'-'z'\n",
        "                init_params=\"\",      # We will set all params\n",
        "                params=\"\"            # Don't train anything\n",
        "            )\n",
        "\n",
        "            model.startprob_ = np.array([1.0] + [0.0] * (length - 1))\n",
        "\n",
        "            transmat = np.zeros((length, length))\n",
        "            for i in range(length - 1):\n",
        "                transmat[i, i + 1] = 1.0\n",
        "            transmat[length - 1, length - 1] = 1.0 # End state loops\n",
        "            model.transmat_ = transmat\n",
        "\n",
        "            # Calculate emission probabilities (P(letter | position))\n",
        "            emission_prob = np.ones((length, 26)) # Add-1 smoothing\n",
        "            for word in word_list:\n",
        "                for i, char in enumerate(word):\n",
        "                    emission_prob[i, ord(char) - ord('a')] += 1\n",
        "\n",
        "            # Normalize\n",
        "            emission_prob /= np.sum(emission_prob, axis=1, keepdims=True)\n",
        "            model.emissionprob_ = emission_prob\n",
        "\n",
        "            self.hmms[length] = model\n",
        "        print(\"HMM training complete.\")\n",
        "\n",
        "    def get_probabilities(self, masked_word, guessed_letters):\n",
        "        \"\"\"\n",
        "        Calculates the probability distribution over the alphabet[cite: 24].\n",
        "\n",
        "        This is the core \"oracle\" function [cite: 19] that estimates\n",
        "        P(letter | masked_word, guessed_letters).\n",
        "        \"\"\"\n",
        "        length = len(masked_word)\n",
        "        if length not in self.words_by_length:\n",
        "            return np.zeros(26) # No words of this length\n",
        "\n",
        "        # 1. Primary Strategy: Word List Filtering\n",
        "        # Find all words that match the current state\n",
        "        pattern = self._get_pattern(masked_word, guessed_letters)\n",
        "        relevant_words = [\n",
        "            word for word in self.words_by_length[length] if pattern.match(word)\n",
        "        ]\n",
        "\n",
        "        probs = np.zeros(26)\n",
        "        available_letters = set(string.ascii_lowercase) - guessed_letters\n",
        "\n",
        "        if relevant_words:\n",
        "            # We found matches. Calculate freqs from this relevant subset.\n",
        "            total_matches = len(relevant_words)\n",
        "            for word in relevant_words:\n",
        "                # Only count letters that are in blank spots\n",
        "                for i, char in enumerate(word):\n",
        "                    if masked_word[i] == '_':\n",
        "                        probs[ord(char) - ord('a')] += 1\n",
        "\n",
        "            # Normalize and mask out already-guessed letters\n",
        "            if np.sum(probs) > 0:\n",
        "                probs /= np.sum(probs)\n",
        "\n",
        "            for i in range(26):\n",
        "                if chr(ord('a') + i) not in available_letters:\n",
        "                    probs[i] = 0\n",
        "\n",
        "            return probs\n",
        "\n",
        "        # 2. Fallback Strategy: Use the HMM\n",
        "        # No relevant words were found (e.g., start of game or rare pattern)\n",
        "        # Use the positional HMM probabilities for the blank spots.\n",
        "        model = self.hmms.get(length)\n",
        "        if model is None:\n",
        "            return np.zeros(26) # Should not happen if trained\n",
        "\n",
        "        probs = np.zeros(26)\n",
        "        for i, char in enumerate(masked_word):\n",
        "            if char == '_':\n",
        "                # Add the probability of all letters at this position\n",
        "                probs += model.emissionprob_[i, :]\n",
        "\n",
        "        # Mask out already-guessed letters\n",
        "        for i in range(26):\n",
        "            if chr(ord('a') + i) not in available_letters:\n",
        "                probs[i] = 0\n",
        "\n",
        "        # Normalize\n",
        "        total_prob = np.sum(probs)\n",
        "        if total_prob > 0:\n",
        "            return probs / total_prob\n",
        "        else:\n",
        "            # No possible letters left (should be a game-over state)\n",
        "            return np.zeros(26)"
      ],
      "metadata": {
        "id": "ZhasT1cMuoO_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning"
      ],
      "metadata": {
        "id": "SfHzKSJZu3dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanEnv:\n",
        "    \"\"\"Implements the Hangman Game Environment for the RL agent.\"\"\"\n",
        "\n",
        "    def __init__(self, word_list, oracle, max_lives=6): # 6 wrong guesses allowed\n",
        "        self.all_words = word_list\n",
        "        self.oracle = oracle\n",
        "        self.max_lives = max_lives\n",
        "        self._reset_game_stats()\n",
        "\n",
        "    def _reset_game_stats(self):\n",
        "        \"\"\"Generates a new game state.\"\"\"\n",
        "        # Words are drawn from the provided corpus only\n",
        "        self.word = random.choice(self.all_words)\n",
        "        self.word_len = len(self.word)\n",
        "        self.masked_word = \"_\" * self.word_len\n",
        "        self.lives_left = self.max_lives\n",
        "        self.guessed_letters = set()\n",
        "        self.game_wrong_guesses = 0\n",
        "        self.game_repeated_guesses = 0\n",
        "        self.game_won = False\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Defines the state representation for the RL agent.\n",
        "        State is a fixed-size vector:\n",
        "        - [0:26]: HMM probability distribution\n",
        "        - [26:52]: Binary vector of guessed letters\n",
        "        - [52]: Normalized number of lives left\n",
        "        - [53]: Normalized number of blank spots\n",
        "        \"\"\"\n",
        "        # Get probability distribution from the HMM Oracle\n",
        "        hmm_probs = self.oracle.get_probabilities(\n",
        "            self.masked_word, self.guessed_letters\n",
        "        )\n",
        "\n",
        "        # Create guessed letters vector\n",
        "        guessed_vec = np.array([\n",
        "            1.0 if c in self.guessed_letters else 0.0\n",
        "            for c in string.ascii_lowercase\n",
        "        ])\n",
        "\n",
        "        # Create metadata\n",
        "        lives_norm = self.lives_left / self.max_lives\n",
        "        blanks_norm = self.masked_word.count('_') / self.word_len\n",
        "\n",
        "        # Concatenate into a single state vector\n",
        "        state = np.concatenate([\n",
        "            hmm_probs,\n",
        "            guessed_vec,\n",
        "            [lives_norm],\n",
        "            [blanks_norm]\n",
        "        ])\n",
        "        return state.astype(np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment and returns the initial state.\"\"\"\n",
        "        self._reset_game_stats()\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes one action (guessing a letter) in the environment.\n",
        "        Action: int 0-25, corresponding to 'a'-'z'.\n",
        "        \"\"\"\n",
        "        letter = chr(ord('a') + action)\n",
        "\n",
        "        done = False\n",
        "        info = {\n",
        "            \"repeated\": 0,\n",
        "            \"wrong\": 0,\n",
        "            \"won\": False,\n",
        "            \"word\": self.word,\n",
        "            \"masked\": self.masked_word\n",
        "        }\n",
        "\n",
        "        # 1. Check for Repeated Guess\n",
        "        if letter in self.guessed_letters:\n",
        "            self.game_repeated_guesses += 1\n",
        "            info[\"repeated\"] = 1\n",
        "            # Reward defined by the scoring formula\n",
        "            reward = -2\n",
        "            return self._get_state(), reward, done, info\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "\n",
        "        # 2. Check for Correct/Wrong Guess\n",
        "        if letter in self.word:\n",
        "            # Correct guess\n",
        "            new_masked_word = list(self.masked_word)\n",
        "            letters_found = 0\n",
        "            for i, char in enumerate(self.word):\n",
        "                if char == letter:\n",
        "                    new_masked_word[i] = letter\n",
        "                    letters_found += 1\n",
        "            self.masked_word = \"\".join(new_masked_word)\n",
        "\n",
        "            # --- REWARD CHANGE ---\n",
        "            # Simpler, more direct reward for progress\n",
        "            reward = 10\n",
        "            info[\"wrong\"] = 0\n",
        "\n",
        "        else:\n",
        "            # Wrong guess\n",
        "            self.lives_left -= 1\n",
        "            info[\"wrong\"] = 1\n",
        "            self.game_wrong_guesses += 1\n",
        "            # Reward defined by the scoring formula\n",
        "            reward = -5\n",
        "\n",
        "        # 3. Check for Game Over (Win or Lose)\n",
        "        if \"_\" not in self.masked_word:\n",
        "            # Game Won\n",
        "            done = True\n",
        "            self.game_won = True\n",
        "            info[\"won\"] = True\n",
        "            # --- REWARD CHANGE ---\n",
        "            # Scaled down to be more balanced with penalties\n",
        "            reward = 100\n",
        "\n",
        "        elif self.lives_left == 0:\n",
        "            # Game Lost\n",
        "            done = True\n",
        "            self.game_won = False\n",
        "            info[\"won\"] = False\n",
        "            # --- REWARD CHANGE ---\n",
        "            # Added a large, explicit penalty for losing\n",
        "            reward = -100\n",
        "\n",
        "        return self._get_state(), reward, done, info"
      ],
      "metadata": {
        "id": "CUwcJAaCu5fz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RL Agent**"
      ],
      "metadata": {
        "id": "FBeM3i2gu7OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Implements a Deep Q-Network (DQN) agent.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99): #\n",
        "        self.state_size = state_size   # 54\n",
        "        self.action_size = action_size # 26\n",
        "\n",
        "        # Experience Replay Buffer\n",
        "        self.memory = deque(maxlen=100000)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma    # Discount factor (increased for long-term planning)\n",
        "\n",
        "        # --- TUNING CHANGE 1 ---\n",
        "        # Lowered learning rate for more stable Q-value convergence\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Exploration-Exploitation Strategy\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "\n",
        "        # --- TUNING CHANGE 2 ---\n",
        "        # Faster decay. We want the agent to stop guessing randomly sooner.\n",
        "        self.epsilon_decay = 0.9992\n",
        "\n",
        "        # Q-Network and Target Network\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Builds the neural network to approximate the Q-function.\"\"\"\n",
        "        model = Sequential([\n",
        "            Input(shape=(self.state_size,)),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear') # Q-values for each action\n",
        "        ])\n",
        "        model.compile(\n",
        "            loss='mse',\n",
        "            optimizer=Adam(learning_rate=self.learning_rate)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"Syncs the Target Network with the weights of the Main Network.\"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Stores an experience tuple in the replay buffer.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Chooses an action using an epsilon-greedy policy.\n",
        "        It *must* mask out (avoid) actions that have already been guessed.\n",
        "        \"\"\"\n",
        "        # Extract the guessed_vec from the state (indices 26 to 52)\n",
        "        guessed_mask = state[0][26:52].astype(bool)\n",
        "\n",
        "        # Get all actions that are *not* guessed\n",
        "        available_actions = [i for i, guessed in enumerate(guessed_mask) if not guessed]\n",
        "\n",
        "        if not available_actions:\n",
        "            # Fallback in case no actions are available\n",
        "            return 0\n",
        "\n",
        "        # Exploration\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "\n",
        "        # Exploitation\n",
        "        act_values = self.model.predict(state, verbose=0)[0]\n",
        "\n",
        "        # Mask out unavailable actions by setting their Q-value to -infinity\n",
        "        masked_act_values = [\n",
        "            act_values[i] if i in available_actions else -np.inf\n",
        "            for i in range(self.action_size)\n",
        "        ]\n",
        "\n",
        "        return np.argmax(masked_act_values)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        \"\"\"Trains the model on a minibatch from the replay buffer.\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return # Not enough samples yet\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        states = np.vstack([t[0] for t in minibatch])\n",
        "        next_states = np.vstack([t[3] for t in minibatch])\n",
        "\n",
        "        # Predict Q-values for current states and next states\n",
        "        q_values_current = self.model.predict(states, verbose=0)\n",
        "        q_values_next_target = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        targets = []\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                # Bellman equation: Q(s,a) = r + gamma * max_a'(Q_target(s',a'))\n",
        "                target = reward + self.gamma * np.amax(q_values_next_target[i])\n",
        "\n",
        "            # Get the Q-values for the batch's i-th state\n",
        "            current_q_target = q_values_current[i]\n",
        "            # Update only the Q-value for the action that was taken\n",
        "            current_q_target[action] = target\n",
        "            targets.append(current_q_target)\n",
        "\n",
        "        # Train the model on the (states, targets) batch\n",
        "        self.model.fit(states, np.array(targets), epochs=1, verbose=0)\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "WPUvIY3Mu-P8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "huHSwGycvMRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # --- 1. Load and Prepare Data ---\n",
        "    all_words = load_corpus(\"corpus.txt\")\n",
        "    if not all_words:\n",
        "        return\n",
        "\n",
        "    words_by_length = group_words_by_length(all_words)\n",
        "\n",
        "    # --- 2. Build HMM Oracle ---\n",
        "    oracle = HMMOracle(words_by_length)\n",
        "    oracle.train() # Train on the corpus\n",
        "\n",
        "    # --- 3. Build Environment and Agent ---\n",
        "    env = HangmanEnv(all_words, oracle, max_lives=6)\n",
        "\n",
        "    STATE_SIZE = 54 # 26 (HMM) + 26 (Guessed) + 1 (Lives) + 1 (Blanks)\n",
        "    ACTION_SIZE = 26 # 'a' - 'z'\n",
        "\n",
        "    # Agent will now use the new hyperparameters\n",
        "    agent = DQNAgent(STATE_SIZE, ACTION_SIZE)\n",
        "\n",
        "    # --- 4. Training Loop ---\n",
        "    # --- TUNING CHANGE 3 ---\n",
        "    # More episodes to allow the smaller learning rate to converge\n",
        "    EPISODES = 6500\n",
        "    BATCH_SIZE = 64\n",
        "    # --- TUNING CHANGE 4 ---\n",
        "    # Update target network *much* less frequently for stability\n",
        "    UPDATE_TARGET_FREQ = 500\n",
        "\n",
        "    print(f\"\\n--- Starting RL Agent Training ({EPISODES} episodes) ---\")\n",
        "    start_time = time.time()\n",
        "    episode_rewards = []\n",
        "\n",
        "    for e in range(1, EPISODES + 1):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, STATE_SIZE])\n",
        "\n",
        "        done = False\n",
        "        total_episode_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                episode_rewards.append(total_episode_reward)\n",
        "\n",
        "        # Train the agent from replay buffer\n",
        "        agent.replay(BATCH_SIZE)\n",
        "\n",
        "        # Update target network\n",
        "        if e % UPDATE_TARGET_FREQ == 0:\n",
        "            agent.update_target_model()\n",
        "            print(f\"--- Target network updated at episode {e} ---\") # Log this\n",
        "\n",
        "        if e % 100 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            print(f\"Episode: {e}/{EPISODES} | \"\n",
        "                  f\"Avg Reward (last 100): {avg_reward:.2f} | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"--- Training Finished in {training_time:.2f}s ---\")\n",
        "\n",
        "    # --- 5. Evaluation ---\n",
        "    EVAL_GAMES = 2000\n",
        "    print(f\"\\n--- Starting Evaluation ({EVAL_GAMES} games) ---\")\n",
        "\n",
        "    # Set agent to exploitation-only mode\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    total_wins = 0\n",
        "    total_wrong_guesses = 0\n",
        "    total_repeated_guesses = 0\n",
        "\n",
        "    # Load the hidden test set\n",
        "    try:\n",
        "        with open(\"test.txt\", 'r') as f:\n",
        "            test_content = f.read()\n",
        "        test_words = re.split(r'\\s+', test_content)\n",
        "        test_words = [word.lower() for word in test_words if word.isalpha()]\n",
        "        env.all_words = test_words # Force env to use test words\n",
        "        print(f\"Loaded {len(test_words)} words from test.txt for evaluation.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: test.txt not found. Evaluating on the training corpus.\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    for g in range(1, EVAL_GAMES + 1):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, STATE_SIZE])\n",
        "\n",
        "        done = False\n",
        "        game_wrong = 0\n",
        "        game_repeated = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            state = np.reshape(next_state, [1, STATE_SIZE])\n",
        "\n",
        "            game_wrong += info[\"wrong\"]\n",
        "            game_repeated += info[\"repeated\"]\n",
        "\n",
        "            if done:\n",
        "                if info[\"won\"]:\n",
        "                    total_wins += 1\n",
        "                total_wrong_guesses += game_wrong\n",
        "                total_repeated_guesses += game_repeated\n",
        "\n",
        "        if g % 200 == 0:\n",
        "            print(f\"Played game {g}/{EVAL_GAMES}...\")\n",
        "\n",
        "    # --- 6. Final Results ---\n",
        "    print(\"\\n--- üèÅ Final Evaluation Results ---\")\n",
        "\n",
        "    success_rate = total_wins / EVAL_GAMES\n",
        "    avg_wrong = total_wrong_guesses / EVAL_GAMES\n",
        "    avg_repeated = total_repeated_guesses / EVAL_GAMES\n",
        "\n",
        "    # Calculate final score based on the formula\n",
        "    final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
        "\n",
        "    print(f\"Total Games Played: {EVAL_GAMES}\")\n",
        "    print(\"\\n--- Averages ---\")\n",
        "    print(f\"Success Rate:         {success_rate * 100:.2f}%\")\n",
        "    print(f\"Avg. Wrong Guesses:   {avg_wrong:.3f}\")\n",
        "    print(f\"Avg. Repeated Guesses: {avg_repeated:.3f}\")\n",
        "\n",
        "    print(\"\\n--- Totals ---\")\n",
        "    print(f\"Total Wins:             {total_wins}\")\n",
        "    print(f\"Total Wrong Guesses:    {total_wrong_guesses}\")\n",
        "    print(f\"Total Repeated Guesses: {total_repeated_guesses}\")\n",
        "\n",
        "    print(\"\\n--- SCORE ---\")\n",
        "    print(f\"Success Points:  ( {success_rate:.3f} * 2000 )   = {success_rate * 2000:,.0f}\")\n",
        "    print(f\"Wrong Penalty:   ( {total_wrong_guesses} * 5 )      = -{total_wrong_guesses * 5:,.0f}\")\n",
        "    print(f\"Repeat Penalty:  ( {total_repeated_guesses} * 2 )      = -{total_repeated_guesses * 2:,.0f}\")\n",
        "    print(f\"**Final Score**:                            = **{final_score:,.0f}**\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AyGi5aorvN4S",
        "outputId": "f7077169-696f-4908-af10-8f84d4823d76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading corpus from corpus.txt...\n",
            "Loaded 49399 unique, valid words.\n",
            "Grouped words into 24 length-based buckets.\n",
            "Training HMMs (positional frequency models)...\n",
            "HMM training complete.\n",
            "\n",
            "--- Starting RL Agent Training (20000 episodes) ---\n",
            "Episode: 100/20000 | Avg Reward (last 100): -100.40 | Epsilon: 0.928\n",
            "Episode: 200/20000 | Avg Reward (last 100): -96.80 | Epsilon: 0.856\n",
            "Episode: 300/20000 | Avg Reward (last 100): -92.10 | Epsilon: 0.790\n",
            "Episode: 400/20000 | Avg Reward (last 100): -95.10 | Epsilon: 0.730\n",
            "--- Target network updated at episode 500 ---\n",
            "Episode: 500/20000 | Avg Reward (last 100): -89.90 | Epsilon: 0.673\n",
            "Episode: 600/20000 | Avg Reward (last 100): -87.55 | Epsilon: 0.622\n",
            "Episode: 700/20000 | Avg Reward (last 100): -86.10 | Epsilon: 0.574\n",
            "Episode: 800/20000 | Avg Reward (last 100): -87.20 | Epsilon: 0.530\n",
            "Episode: 900/20000 | Avg Reward (last 100): -81.55 | Epsilon: 0.489\n",
            "--- Target network updated at episode 1000 ---\n",
            "Episode: 1000/20000 | Avg Reward (last 100): -78.40 | Epsilon: 0.451\n",
            "Episode: 1100/20000 | Avg Reward (last 100): -73.30 | Epsilon: 0.417\n",
            "Episode: 1200/20000 | Avg Reward (last 100): -63.55 | Epsilon: 0.385\n",
            "Episode: 1300/20000 | Avg Reward (last 100): -68.20 | Epsilon: 0.355\n",
            "Episode: 1400/20000 | Avg Reward (last 100): -56.90 | Epsilon: 0.328\n",
            "--- Target network updated at episode 1500 ---\n",
            "Episode: 1500/20000 | Avg Reward (last 100): -45.60 | Epsilon: 0.302\n",
            "Episode: 1600/20000 | Avg Reward (last 100): -56.20 | Epsilon: 0.279\n",
            "Episode: 1700/20000 | Avg Reward (last 100): -53.85 | Epsilon: 0.258\n",
            "Episode: 1800/20000 | Avg Reward (last 100): -43.90 | Epsilon: 0.238\n",
            "Episode: 1900/20000 | Avg Reward (last 100): -40.30 | Epsilon: 0.220\n",
            "--- Target network updated at episode 2000 ---\n",
            "Episode: 2000/20000 | Avg Reward (last 100): -47.05 | Epsilon: 0.203\n",
            "Episode: 2100/20000 | Avg Reward (last 100): -26.50 | Epsilon: 0.187\n",
            "Episode: 2200/20000 | Avg Reward (last 100): -14.80 | Epsilon: 0.173\n",
            "Episode: 2300/20000 | Avg Reward (last 100): 0.35 | Epsilon: 0.159\n",
            "Episode: 2400/20000 | Avg Reward (last 100): -28.10 | Epsilon: 0.147\n",
            "--- Target network updated at episode 2500 ---\n",
            "Episode: 2500/20000 | Avg Reward (last 100): -5.00 | Epsilon: 0.136\n",
            "Episode: 2600/20000 | Avg Reward (last 100): 36.65 | Epsilon: 0.125\n",
            "Episode: 2700/20000 | Avg Reward (last 100): -5.70 | Epsilon: 0.116\n",
            "Episode: 2800/20000 | Avg Reward (last 100): -3.05 | Epsilon: 0.107\n",
            "Episode: 2900/20000 | Avg Reward (last 100): -6.65 | Epsilon: 0.099\n",
            "--- Target network updated at episode 3000 ---\n",
            "Episode: 3000/20000 | Avg Reward (last 100): 41.75 | Epsilon: 0.091\n",
            "Episode: 3100/20000 | Avg Reward (last 100): 36.75 | Epsilon: 0.084\n",
            "Episode: 3200/20000 | Avg Reward (last 100): 65.90 | Epsilon: 0.078\n",
            "Episode: 3300/20000 | Avg Reward (last 100): 42.05 | Epsilon: 0.072\n",
            "Episode: 3400/20000 | Avg Reward (last 100): 44.90 | Epsilon: 0.066\n",
            "--- Target network updated at episode 3500 ---\n",
            "Episode: 3500/20000 | Avg Reward (last 100): 45.30 | Epsilon: 0.061\n",
            "Episode: 3600/20000 | Avg Reward (last 100): 88.75 | Epsilon: 0.056\n",
            "Episode: 3700/20000 | Avg Reward (last 100): 53.50 | Epsilon: 0.052\n",
            "Episode: 3800/20000 | Avg Reward (last 100): 74.55 | Epsilon: 0.048\n",
            "Episode: 3900/20000 | Avg Reward (last 100): 75.25 | Epsilon: 0.044\n",
            "--- Target network updated at episode 4000 ---\n",
            "Episode: 4000/20000 | Avg Reward (last 100): 57.15 | Epsilon: 0.041\n",
            "Episode: 4100/20000 | Avg Reward (last 100): 79.75 | Epsilon: 0.038\n",
            "Episode: 4200/20000 | Avg Reward (last 100): 73.00 | Epsilon: 0.035\n",
            "Episode: 4300/20000 | Avg Reward (last 100): 80.45 | Epsilon: 0.032\n",
            "Episode: 4400/20000 | Avg Reward (last 100): 81.80 | Epsilon: 0.030\n",
            "--- Target network updated at episode 4500 ---\n",
            "Episode: 4500/20000 | Avg Reward (last 100): 95.25 | Epsilon: 0.027\n",
            "Episode: 4600/20000 | Avg Reward (last 100): 82.65 | Epsilon: 0.025\n",
            "Episode: 4700/20000 | Avg Reward (last 100): 75.95 | Epsilon: 0.023\n",
            "Episode: 4800/20000 | Avg Reward (last 100): 85.15 | Epsilon: 0.022\n",
            "Episode: 4900/20000 | Avg Reward (last 100): 84.90 | Epsilon: 0.020\n",
            "--- Target network updated at episode 5000 ---\n",
            "Episode: 5000/20000 | Avg Reward (last 100): 91.10 | Epsilon: 0.018\n",
            "Episode: 5100/20000 | Avg Reward (last 100): 88.30 | Epsilon: 0.017\n",
            "Episode: 5200/20000 | Avg Reward (last 100): 87.25 | Epsilon: 0.016\n",
            "Episode: 5300/20000 | Avg Reward (last 100): 84.10 | Epsilon: 0.014\n",
            "Episode: 5400/20000 | Avg Reward (last 100): 80.05 | Epsilon: 0.013\n",
            "--- Target network updated at episode 5500 ---\n",
            "Episode: 5500/20000 | Avg Reward (last 100): 92.90 | Epsilon: 0.012\n",
            "Episode: 5600/20000 | Avg Reward (last 100): 85.85 | Epsilon: 0.011\n",
            "Episode: 5700/20000 | Avg Reward (last 100): 93.10 | Epsilon: 0.010\n",
            "Episode: 5800/20000 | Avg Reward (last 100): 120.00 | Epsilon: 0.010\n",
            "Episode: 5900/20000 | Avg Reward (last 100): 98.05 | Epsilon: 0.010\n",
            "--- Target network updated at episode 6000 ---\n",
            "Episode: 6000/20000 | Avg Reward (last 100): 101.85 | Epsilon: 0.010\n",
            "Episode: 6100/20000 | Avg Reward (last 100): 99.00 | Epsilon: 0.010\n",
            "Episode: 6200/20000 | Avg Reward (last 100): 104.50 | Epsilon: 0.010\n",
            "Episode: 6300/20000 | Avg Reward (last 100): 97.60 | Epsilon: 0.010\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4210185867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4210185867.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATE_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-505275087.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Exploitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Mask out unavailable actions by setting their Q-value to -infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    503\u001b[0m     ):\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrab_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             dataset = dataset.map(\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mgrab_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_unbounded_threadpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_unbounded_threadpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[1;32m   5864\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5865\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5866\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5867\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ParallelMapDatasetV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5868\u001b[0m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
