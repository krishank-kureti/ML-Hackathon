{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict, Counter, deque\n",
        "import random\n",
        "from typing import List, Set, Tuple, Dict\n",
        "import re\n",
        "\n",
        "# ADD THIS FUNCTION\n",
        "def _create_nested_float_dict():\n",
        "    return defaultdict(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R37ipAgEuN7f",
        "outputId": "53b101a8-5a96-4ca0-e678-f9d62ca92af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Training Advanced HMM\n",
            "Training Advanced HMM with word filtering...\n",
            "Trained models for 24 word lengths\n",
            "Total vocabulary: 49979 words\n",
            "\n",
            "Step 2: Loading training and test data\n",
            "Training set: 49979 words\n",
            "Test set: 2000 words\n",
            "\n",
            "Step 3: Training Enhanced RL Agent\n",
            "\n",
            "Training RL agent for 15000 episodes...\n",
            "Episode 1000/15000 - Win Rate: 0.497 | Recent 100: 0.630 | Avg Wrong: 4.73 | Epsilon: 0.606\n",
            "Episode 2000/15000 - Win Rate: 0.633 | Recent 100: 0.850 | Avg Wrong: 4.15 | Epsilon: 0.368\n",
            "Episode 3000/15000 - Win Rate: 0.715 | Recent 100: 0.860 | Avg Wrong: 3.73 | Epsilon: 0.223\n",
            "Episode 4000/15000 - Win Rate: 0.772 | Recent 100: 0.940 | Avg Wrong: 3.38 | Epsilon: 0.135\n",
            "Episode 5000/15000 - Win Rate: 0.807 | Recent 100: 0.940 | Avg Wrong: 3.10 | Epsilon: 0.082\n",
            "Episode 6000/15000 - Win Rate: 0.832 | Recent 100: 0.970 | Avg Wrong: 2.90 | Epsilon: 0.050\n",
            "Episode 7000/15000 - Win Rate: 0.848 | Recent 100: 0.910 | Avg Wrong: 2.76 | Epsilon: 0.030\n",
            "Episode 8000/15000 - Win Rate: 0.860 | Recent 100: 0.920 | Avg Wrong: 2.65 | Epsilon: 0.018\n",
            "Episode 9000/15000 - Win Rate: 0.868 | Recent 100: 0.920 | Avg Wrong: 2.56 | Epsilon: 0.011\n",
            "Episode 10000/15000 - Win Rate: 0.875 | Recent 100: 0.940 | Avg Wrong: 2.48 | Epsilon: 0.010\n",
            "Episode 11000/15000 - Win Rate: 0.880 | Recent 100: 0.940 | Avg Wrong: 2.43 | Epsilon: 0.010\n",
            "Episode 12000/15000 - Win Rate: 0.886 | Recent 100: 0.980 | Avg Wrong: 2.37 | Epsilon: 0.010\n",
            "Episode 13000/15000 - Win Rate: 0.891 | Recent 100: 0.960 | Avg Wrong: 2.33 | Epsilon: 0.010\n",
            "Episode 14000/15000 - Win Rate: 0.895 | Recent 100: 0.970 | Avg Wrong: 2.29 | Epsilon: 0.010\n",
            "Episode 15000/15000 - Win Rate: 0.898 | Recent 100: 0.910 | Avg Wrong: 2.26 | Epsilon: 0.010\n",
            "\n",
            "Training complete! Final win rate: 0.898\n",
            "\n",
            "Step 4: Final Evaluation\n",
            "\n",
            "Evaluating on 2000 test games...\n",
            "Evaluated 500/2000 games...\n",
            "Evaluated 1000/2000 games...\n",
            "Evaluated 1500/2000 games...\n",
            "Evaluated 2000/2000 games...\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "Games Played: 2000\n",
            "Wins: 509\n",
            "Success Rate: 25.45%\n",
            "Total Wrong Guesses: 10767\n",
            "Average Wrong Guesses: 5.38\n",
            "Total Repeated Guesses: 0\n",
            "\n",
            "FINAL SCORE: -53326.00\n",
            "============================================================\n",
            "\n",
            "Saving model...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "Can't get local object 'AdvancedHMMHangman._train_positional_model.<locals>.<lambda>'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3483077509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSaving model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hangman_agent_optimized.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'hmm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved to 'hangman_agent_optimized.pkl'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get local object 'AdvancedHMMHangman._train_positional_model.<locals>.<lambda>'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict, Counter, deque\n",
        "import random\n",
        "from typing import List, Set, Tuple, Dict\n",
        "import re\n",
        "\n",
        "class AdvancedHMMHangman:\n",
        "    \"\"\"Enhanced HMM with word filtering for superior prediction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_length_models = {}\n",
        "        self.words_by_length = defaultdict(list)\n",
        "        self.letter_frequencies = None\n",
        "        self.alphabet = set('abcdefghijklmnopqrstuvwxyz')\n",
        "        self.bigram_freq = defaultdict(lambda: defaultdict(int))\n",
        "        self.trigram_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def train(self, corpus_file: str):\n",
        "        \"\"\"Train enhanced HMM with multiple pattern sources\"\"\"\n",
        "        print(\"Training Advanced HMM with word filtering...\")\n",
        "\n",
        "        with open(corpus_file, 'r') as f:\n",
        "            words = [line.strip().lower() for line in f if line.strip()]\n",
        "\n",
        "        # Group words by length\n",
        "        for word in words:\n",
        "            if word.isalpha():\n",
        "                self.words_by_length[len(word)].append(word)\n",
        "\n",
        "        # Global letter frequencies\n",
        "        all_letters = ''.join(words)\n",
        "        letter_counts = Counter(all_letters)\n",
        "        total = sum(letter_counts.values())\n",
        "        self.letter_frequencies = {l: letter_counts[l] / total for l in self.alphabet}\n",
        "\n",
        "        # Build n-gram models\n",
        "        for word in words:\n",
        "            if not word.isalpha():\n",
        "                continue\n",
        "            # Bigrams\n",
        "            for i in range(len(word) - 1):\n",
        "                self.bigram_freq[word[i]][word[i+1]] += 1\n",
        "            # Trigrams\n",
        "            for i in range(len(word) - 2):\n",
        "                context = word[i:i+2]\n",
        "                self.trigram_freq[context][word[i+2]] += 1\n",
        "\n",
        "        # Normalize bigrams\n",
        "        for c1 in self.bigram_freq:\n",
        "            total = sum(self.bigram_freq[c1].values())\n",
        "            for c2 in self.bigram_freq[c1]:\n",
        "                self.bigram_freq[c1][c2] /= total\n",
        "\n",
        "        # Normalize trigrams\n",
        "        for context in self.trigram_freq:\n",
        "            total = sum(self.trigram_freq[context].values())\n",
        "            for letter in self.trigram_freq[context]:\n",
        "                self.trigram_freq[context][letter] /= total\n",
        "\n",
        "        # Train positional models\n",
        "        for length, word_list in self.words_by_length.items():\n",
        "            self.word_length_models[length] = self._train_positional_model(word_list, length)\n",
        "\n",
        "        print(f\"Trained models for {len(self.word_length_models)} word lengths\")\n",
        "        print(f\"Total vocabulary: {sum(len(v) for v in self.words_by_length.values())} words\")\n",
        "\n",
        "    def _train_positional_model(self, words: List[str], length: int) -> Dict:\n",
        "        \"\"\"Train position-based probability model\"\"\"\n",
        "        model = {'position_probs': defaultdict(_create_nested_float_dict)}\n",
        "\n",
        "        for word in words:\n",
        "            for pos, letter in enumerate(word):\n",
        "                model['position_probs'][pos][letter] += 1\n",
        "\n",
        "        # Normalize with smoothing\n",
        "        for pos in model['position_probs']:\n",
        "            total = sum(model['position_probs'][pos].values()) + 26  # Add-one smoothing\n",
        "            for letter in self.alphabet:\n",
        "                count = model['position_probs'][pos].get(letter, 0) + 1\n",
        "                model['position_probs'][pos][letter] = count / total\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _build_regex_pattern(self, masked_word: str, guessed_letters: Set[str]) -> re.Pattern:\n",
        "        \"\"\"Build regex pattern for word filtering\"\"\"\n",
        "        wrong_letters = guessed_letters - set(masked_word)\n",
        "        pattern_parts = []\n",
        "\n",
        "        for char in masked_word:\n",
        "            if char == '_':\n",
        "                if wrong_letters:\n",
        "                    negation = ''.join(sorted(wrong_letters))\n",
        "                    pattern_parts.append(f\"[^{negation}]\")\n",
        "                else:\n",
        "                    pattern_parts.append('.')\n",
        "            else:\n",
        "                pattern_parts.append(char)\n",
        "\n",
        "        return re.compile('^' + ''.join(pattern_parts) + '$')\n",
        "\n",
        "    def predict_probabilities(self, masked_word: str, guessed_letters: Set[str]) -> Dict[str, float]:\n",
        "        \"\"\"Predict using hybrid word-filtering + statistical approach\"\"\"\n",
        "        length = len(masked_word)\n",
        "        remaining_letters = self.alphabet - guessed_letters\n",
        "\n",
        "        if not remaining_letters:\n",
        "            return {l: 0.0 for l in self.alphabet}\n",
        "\n",
        "        # STRATEGY 1: Word List Filtering (Most Powerful)\n",
        "        if length in self.words_by_length:\n",
        "            pattern = self._build_regex_pattern(masked_word, guessed_letters)\n",
        "            matching_words = [w for w in self.words_by_length[length] if pattern.match(w)]\n",
        "\n",
        "            if matching_words:\n",
        "                letter_counts = Counter()\n",
        "                for word in matching_words:\n",
        "                    for i, char in enumerate(word):\n",
        "                        if masked_word[i] == '_':\n",
        "                            letter_counts[char] += 1\n",
        "\n",
        "                total = sum(letter_counts.values())\n",
        "                if total > 0:\n",
        "                    probs = {}\n",
        "                    for letter in remaining_letters:\n",
        "                        probs[letter] = letter_counts.get(letter, 0) / total\n",
        "                    return probs\n",
        "\n",
        "        # STRATEGY 2: Combined Statistical Approach\n",
        "        probs = defaultdict(float)\n",
        "\n",
        "        for letter in remaining_letters:\n",
        "            score = 0.0\n",
        "            weight = 0.0\n",
        "\n",
        "            # Positional probabilities\n",
        "            if length in self.word_length_models:\n",
        "                model = self.word_length_models[length]\n",
        "                for pos, char in enumerate(masked_word):\n",
        "                    if char == '_':\n",
        "                        pos_prob = model['position_probs'][pos].get(letter, 0.001)\n",
        "                        score += pos_prob * 3.0\n",
        "                        weight += 3.0\n",
        "\n",
        "            # Context-based (bigram/trigram)\n",
        "            for pos, char in enumerate(masked_word):\n",
        "                if char == '_':\n",
        "                    # Left context (bigram)\n",
        "                    if pos > 0 and masked_word[pos-1] != '_':\n",
        "                        left = masked_word[pos-1]\n",
        "                        bigram_prob = self.bigram_freq[left].get(letter, 0.001)\n",
        "                        score += bigram_prob * 2.5\n",
        "                        weight += 2.5\n",
        "\n",
        "                    # Right context (bigram)\n",
        "                    if pos < length - 1 and masked_word[pos+1] != '_':\n",
        "                        # Reverse lookup\n",
        "                        right = masked_word[pos+1]\n",
        "                        bigram_prob = self.bigram_freq[letter].get(right, 0.001)\n",
        "                        score += bigram_prob * 2.5\n",
        "                        weight += 2.5\n",
        "\n",
        "                    # Trigram context\n",
        "                    if pos > 0 and pos < length - 1:\n",
        "                        left = masked_word[pos-1] if masked_word[pos-1] != '_' else None\n",
        "                        right = masked_word[pos+1] if masked_word[pos+1] != '_' else None\n",
        "                        if left and right:\n",
        "                            context = left + right\n",
        "                            trigram_prob = self.trigram_freq.get(context, {}).get(letter, 0.001)\n",
        "                            score += trigram_prob * 2.0\n",
        "                            weight += 2.0\n",
        "\n",
        "            # Global frequency (baseline)\n",
        "            score += self.letter_frequencies.get(letter, 0.001)\n",
        "            weight += 1.0\n",
        "\n",
        "            probs[letter] = score / weight if weight > 0 else 0.001\n",
        "\n",
        "        # Normalize\n",
        "        total = sum(probs.values())\n",
        "        return {l: probs[l] / total for l in probs} if total > 0 else {l: 1.0/len(remaining_letters) for l in remaining_letters}\n",
        "\n",
        "\n",
        "class HangmanEnvironment:\n",
        "    \"\"\"Optimized Hangman environment\"\"\"\n",
        "\n",
        "    def __init__(self, word_list: List[str], max_wrong: int = 6):\n",
        "        self.word_list = word_list\n",
        "        self.max_wrong = max_wrong\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, word: str = None):\n",
        "        self.target_word = word if word else random.choice(self.word_list)\n",
        "        self.masked_word = '_' * len(self.target_word)\n",
        "        self.guessed_letters = set()\n",
        "        self.wrong_guesses = 0\n",
        "        self.game_over = False\n",
        "        self.won = False\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self) -> Dict:\n",
        "        return {\n",
        "            'masked_word': self.masked_word,\n",
        "            'guessed_letters': self.guessed_letters.copy(),\n",
        "            'wrong_guesses': self.wrong_guesses,\n",
        "            'lives_left': self.max_wrong - self.wrong_guesses,\n",
        "            'progress': 1.0 - (self.masked_word.count('_') / len(self.target_word))\n",
        "        }\n",
        "\n",
        "    def step(self, letter: str) -> Tuple[Dict, float, bool]:\n",
        "        letter = letter.lower()\n",
        "\n",
        "        if letter in self.guessed_letters:\n",
        "            return self.get_state(), -20, self.game_over\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "\n",
        "        if letter in self.target_word:\n",
        "            new_masked = list(self.masked_word)\n",
        "            count = 0\n",
        "            for i, char in enumerate(self.target_word):\n",
        "                if char == letter:\n",
        "                    new_masked[i] = letter\n",
        "                    count += 1\n",
        "            self.masked_word = ''.join(new_masked)\n",
        "\n",
        "            if '_' not in self.masked_word:\n",
        "                self.game_over = True\n",
        "                self.won = True\n",
        "                reward = 200 - (self.wrong_guesses * 10)\n",
        "            else:\n",
        "                reward = 15 * count  # Reward proportional to letters revealed\n",
        "        else:\n",
        "            self.wrong_guesses += 1\n",
        "            reward = -25\n",
        "\n",
        "            if self.wrong_guesses >= self.max_wrong:\n",
        "                self.game_over = True\n",
        "                self.won = False\n",
        "                reward = -100\n",
        "\n",
        "        return self.get_state(), reward, self.game_over\n",
        "\n",
        "\n",
        "class ImprovedRLAgent:\n",
        "    \"\"\"Enhanced RL agent with better exploration and HMM integration\"\"\"\n",
        "\n",
        "    def __init__(self, hmm: AdvancedHMMHangman, alpha: float = 0.15, gamma: float = 0.95):\n",
        "        self.hmm = hmm\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.9995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.alphabet = set('abcdefghijklmnopqrstuvwxyz')\n",
        "\n",
        "        # Experience replay for better learning\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.visit_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def _state_key(self, state: Dict) -> str:\n",
        "        return f\"{state['masked_word']}:{state['lives_left']}\"\n",
        "\n",
        "    def choose_action(self, state: Dict, training: bool = True) -> str:\n",
        "        available_letters = self.alphabet - state['guessed_letters']\n",
        "\n",
        "        if not available_letters:\n",
        "            return random.choice(list(self.alphabet))\n",
        "\n",
        "        # Get HMM probabilities\n",
        "        hmm_probs = self.hmm.predict_probabilities(state['masked_word'], state['guessed_letters'])\n",
        "\n",
        "        # Exploration with smart bias toward HMM\n",
        "        if training and random.random() < self.epsilon:\n",
        "            # Weighted random selection (favor HMM suggestions even during exploration)\n",
        "            if random.random() < 0.5:\n",
        "                weights = [hmm_probs.get(l, 0.001) for l in available_letters]\n",
        "                total = sum(weights)\n",
        "                weights = [w/total for w in weights]\n",
        "                return np.random.choice(list(available_letters), p=weights)\n",
        "            else:\n",
        "                return random.choice(list(available_letters))\n",
        "\n",
        "        # Exploitation: Combine Q-values with HMM\n",
        "        state_key = self._state_key(state)\n",
        "        best_letter = None\n",
        "        best_score = float('-inf')\n",
        "\n",
        "        for letter in available_letters:\n",
        "            q_value = self.q_table[state_key].get(letter, 0)\n",
        "            hmm_prob = hmm_probs.get(letter, 0.001)\n",
        "\n",
        "            # UCB-style exploration bonus\n",
        "            visit_count = self.visit_counts[state_key][letter]\n",
        "            exploration_bonus = np.sqrt(2 * np.log(sum(self.visit_counts[state_key].values()) + 1) / (visit_count + 1))\n",
        "\n",
        "            # Combine signals\n",
        "            combined_score = q_value + (hmm_prob * 100) + (exploration_bonus * 5 if training else 0)\n",
        "\n",
        "            if combined_score > best_score:\n",
        "                best_score = combined_score\n",
        "                best_letter = letter\n",
        "\n",
        "        return best_letter if best_letter else random.choice(list(available_letters))\n",
        "\n",
        "    def update(self, state: Dict, action: str, reward: float, next_state: Dict, done: bool):\n",
        "        state_key = self._state_key(state)\n",
        "        next_state_key = self._state_key(next_state)\n",
        "\n",
        "        # Track visits\n",
        "        self.visit_counts[state_key][action] += 1\n",
        "\n",
        "        # Q-learning update\n",
        "        current_q = self.q_table[state_key][action]\n",
        "\n",
        "        if done:\n",
        "            max_next_q = 0\n",
        "        else:\n",
        "            available_letters = self.alphabet - next_state['guessed_letters']\n",
        "            max_next_q = max([self.q_table[next_state_key].get(l, 0) for l in available_letters]) if available_letters else 0\n",
        "\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
        "        self.q_table[state_key][action] = new_q\n",
        "\n",
        "        # Store experience\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay_batch(self, batch_size: int = 32):\n",
        "        \"\"\"Learn from random experiences\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            self.update(state, action, reward, next_state, done)\n",
        "\n",
        "    def train(self, env: HangmanEnvironment, episodes: int = 15000):\n",
        "        print(f\"\\nTraining RL agent for {episodes} episodes...\")\n",
        "\n",
        "        wins = 0\n",
        "        total_wrong = 0\n",
        "        recent_wins = deque(maxlen=100)\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state, training=True)\n",
        "                next_state, reward, done = env.step(action)\n",
        "                self.update(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "            if env.won:\n",
        "                wins += 1\n",
        "                recent_wins.append(1)\n",
        "            else:\n",
        "                recent_wins.append(0)\n",
        "            total_wrong += env.wrong_guesses\n",
        "\n",
        "            # Batch replay every 10 episodes\n",
        "            if episode % 10 == 0:\n",
        "                self.replay_batch()\n",
        "\n",
        "            # Decay epsilon\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            if (episode + 1) % 1000 == 0:\n",
        "                win_rate = wins / (episode + 1)\n",
        "                recent_wr = sum(recent_wins) / len(recent_wins) if recent_wins else 0\n",
        "                avg_wrong = total_wrong / (episode + 1)\n",
        "                print(f\"Episode {episode + 1}/{episodes} - Win Rate: {win_rate:.3f} | \"\n",
        "                      f\"Recent 100: {recent_wr:.3f} | Avg Wrong: {avg_wrong:.2f} | Epsilon: {self.epsilon:.3f}\")\n",
        "\n",
        "        final_win_rate = wins / episodes\n",
        "        print(f\"\\nTraining complete! Final win rate: {final_win_rate:.3f}\")\n",
        "\n",
        "\n",
        "def evaluate_agent(agent: ImprovedRLAgent, test_words: List[str], max_wrong: int = 6, verbose: bool = True) -> Dict:\n",
        "    \"\"\"Evaluate agent on test set\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\nEvaluating on {len(test_words)} test games...\")\n",
        "\n",
        "    wins = 0\n",
        "    total_wrong = 0\n",
        "    total_repeated = 0\n",
        "\n",
        "    for i, word in enumerate(test_words):\n",
        "        env = HangmanEnvironment([word], max_wrong)\n",
        "        state = env.reset(word)\n",
        "        guessed_this_game = set()\n",
        "\n",
        "        while not env.game_over:\n",
        "            action = agent.choose_action(state, training=False)\n",
        "\n",
        "            if action in guessed_this_game:\n",
        "                total_repeated += 1\n",
        "            guessed_this_game.add(action)\n",
        "\n",
        "            state, reward, done = env.step(action)\n",
        "\n",
        "        if env.won:\n",
        "            wins += 1\n",
        "        total_wrong += env.wrong_guesses\n",
        "\n",
        "        if verbose and (i + 1) % 500 == 0:\n",
        "            print(f\"Evaluated {i + 1}/{len(test_words)} games...\")\n",
        "\n",
        "    success_rate = wins / len(test_words)\n",
        "    final_score = (success_rate * len(test_words)) - (total_wrong * 5) - (total_repeated * 2)\n",
        "\n",
        "    results = {\n",
        "        'games_played': len(test_words),\n",
        "        'wins': wins,\n",
        "        'success_rate': success_rate,\n",
        "        'total_wrong': total_wrong,\n",
        "        'avg_wrong': total_wrong / len(test_words),\n",
        "        'total_repeated': total_repeated,\n",
        "        'final_score': final_score\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Games Played: {results['games_played']}\")\n",
        "        print(f\"Wins: {results['wins']}\")\n",
        "        print(f\"Success Rate: {results['success_rate']:.2%}\")\n",
        "        print(f\"Total Wrong Guesses: {results['total_wrong']}\")\n",
        "        print(f\"Average Wrong Guesses: {results['avg_wrong']:.2f}\")\n",
        "        print(f\"Total Repeated Guesses: {results['total_repeated']}\")\n",
        "        print(f\"\\nFINAL SCORE: {results['final_score']:.2f}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    CORPUS_FILE = \"/content/corpus.txt\"\n",
        "    TEST_FILE = \"/content/test.txt\"\n",
        "    TRAINING_EPISODES = 15000\n",
        "\n",
        "    # Step 1: Train HMM\n",
        "    print(\"Step 1: Training Advanced HMM\")\n",
        "    hmm = AdvancedHMMHangman()\n",
        "    hmm.train(CORPUS_FILE)\n",
        "\n",
        "    # Step 2: Load data\n",
        "    print(\"\\nStep 2: Loading training and test data\")\n",
        "    with open(CORPUS_FILE, 'r') as f:\n",
        "        train_words = [line.strip().lower() for line in f if line.strip() and line.strip().isalpha()]\n",
        "\n",
        "    with open(TEST_FILE, 'r') as f:\n",
        "        test_words = [line.strip().lower() for line in f if line.strip() and line.strip().isalpha()]\n",
        "\n",
        "    print(f\"Training set: {len(train_words)} words\")\n",
        "    print(f\"Test set: {len(test_words)} words\")\n",
        "\n",
        "    # Step 3: Train RL agent\n",
        "    print(\"\\nStep 3: Training Enhanced RL Agent\")\n",
        "    env = HangmanEnvironment(train_words)\n",
        "    agent = ImprovedRLAgent(hmm)\n",
        "    agent.train(env, episodes=TRAINING_EPISODES)\n",
        "\n",
        "    # Step 4: Evaluate\n",
        "    print(\"\\nStep 4: Final Evaluation\")\n",
        "    results = evaluate_agent(agent, test_words)\n",
        "\n",
        "    # Step 5: Save model\n",
        "    print(\"\\nSaving model...\")\n",
        "    with open('hangman_agent_optimized.pkl', 'wb') as f:\n",
        "        pickle.dump({'hmm': hmm, 'agent': agent}, f)\n",
        "    print(\"Model saved to 'hangman_agent_optimized.pkl'\")\n",
        "\n",
        "    # Additional analysis\n",
        "    if results['success_rate'] >= 0.80:\n",
        "        print(\"\\nðŸŽ‰ SUCCESS! Achieved 80%+ win rate!\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸  Current win rate: {results['success_rate']:.1%}. Target: 80%+\")\n",
        "        print(\"Consider: Longer training, hyperparameter tuning, or more sophisticated features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gl0ZiL82BkDb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "\n",
        "def play_game_demo(agent_to_demo, word_to_play):\n",
        "    \"\"\"\n",
        "    Plays a single game of Hangman using the trained agent and prints the steps.\n",
        "\n",
        "    Assumes 'HangmanEnvironment' class is already defined in the notebook.\n",
        "    \"\"\"\n",
        "    print(\"=\"*40)\n",
        "    print(f\"DEMO: Agent will try to guess the word!\")\n",
        "    print(f\"The word has {len(word_to_play)} letters.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Use the HangmanEnvironment class already defined in your notebook\n",
        "    game_env = HangmanEnvironment([word_to_play], max_wrong=6)\n",
        "    state = game_env.reset(word=word_to_play)\n",
        "\n",
        "    # Give a 2-second pause to start\n",
        "    time.sleep(2)\n",
        "\n",
        "    while not game_env.game_over:\n",
        "        # 1. Get current state and print it\n",
        "        masked = ' '.join(list(state['masked_word'])) # Add spaces for readability\n",
        "        lives = state['lives_left']\n",
        "        guessed = sorted(list(state['guessed_letters']))\n",
        "\n",
        "        print(f\"Word:  {masked}\")\n",
        "        print(f\"Lives: {lives} â¤ï¸\")\n",
        "        print(f\"Guessed: {' '.join(guessed)}\")\n",
        "\n",
        "        # 2. Agent chooses action\n",
        "        # We pass training=False so it doesn't explore and only uses its policy\n",
        "        action = agent_to_demo.choose_action(state, training=False)\n",
        "\n",
        "        print(f\"\\nAgent guesses: '{action}'\")\n",
        "        sys.stdout.flush() # Make sure text prints before sleep\n",
        "\n",
        "        # 3. Take step in the environment\n",
        "        state, reward, done = game_env.step(action)\n",
        "\n",
        "        # 4. Print result of the guess\n",
        "        if reward > 0 and not done:\n",
        "            print(\"...Correct!\")\n",
        "        elif reward < -20: # This was a wrong guess\n",
        "            print(\"...Incorrect!\")\n",
        "        elif reward == -20: # This was a repeated guess\n",
        "             print(\"...Already guessed that!\")\n",
        "\n",
        "        time.sleep(1.5) # Pause for 1.5 seconds to make it readable\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # Game is over, print final result\n",
        "    print(\"=\"*40)\n",
        "    print(\"GAME OVER\")\n",
        "    if game_env.won:\n",
        "        print(f\"ðŸŽ‰ Agent WON! The word was '{word_to_play}'\")\n",
        "    else:\n",
        "        print(f\"ðŸ˜¢ Agent LOST. The word was '{word_to_play}'\")\n",
        "    print(f\"Final state: {game_env.masked_word}\")\n",
        "    print(f\"Total wrong guesses: {game_env.wrong_guesses}\")\n",
        "    print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOgJ0q4RCWIy",
        "outputId": "d6ecccf2-118b-4ad4-e93d-003f8e7a806c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "DEMO: Agent will try to guess the word!\n",
            "The word has 11 letters.\n",
            "========================================\n",
            "Word:  _ _ _ _ _ _ _ _ _ _ _\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: \n",
            "\n",
            "Agent guesses: 'e'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ _ _ e _ _ _\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: e\n",
            "\n",
            "Agent guesses: 'n'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ _ _ e n _ _\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: e n\n",
            "\n",
            "Agent guesses: 'c'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ _ _ e n _ c\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: c e n\n",
            "\n",
            "Agent guesses: 'i'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ _ _ e n i c\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: c e i n\n",
            "\n",
            "Agent guesses: 'o'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ o _ e n i c\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: c e i n o\n",
            "\n",
            "Agent guesses: 'g'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  _ _ _ _ _ o g e n i c\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: c e g i n o\n",
            "\n",
            "Agent guesses: 't'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  t _ _ _ _ o g e n i c\n",
            "Lives: 6 â¤ï¸\n",
            "Guessed: c e g i n o t\n",
            "\n",
            "Agent guesses: 'r'\n",
            "...Incorrect!\n",
            "--------------------\n",
            "Word:  t _ _ _ _ o g e n i c\n",
            "Lives: 5 â¤ï¸\n",
            "Guessed: c e g i n o r t\n",
            "\n",
            "Agent guesses: 'a'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  t _ a _ _ o g e n i c\n",
            "Lives: 5 â¤ï¸\n",
            "Guessed: a c e g i n o r t\n",
            "\n",
            "Agent guesses: 'l'\n",
            "...Correct!\n",
            "--------------------\n",
            "Word:  t _ a l l o g e n i c\n",
            "Lives: 5 â¤ï¸\n",
            "Guessed: a c e g i l n o r t\n",
            "\n",
            "Agent guesses: 'h'\n",
            "--------------------\n",
            "========================================\n",
            "GAME OVER\n",
            "ðŸŽ‰ Agent WON! The word was 'thallogenic'\n",
            "Final state: thallogenic\n",
            "Total wrong guesses: 1\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# The 'test_words' and 'agent' variables should still be in memory\n",
        "# from the previous cells you ran.\n",
        "\n",
        "if 'test_words' in locals() and 'agent' in locals():\n",
        "    # Pick a random word from the test set\n",
        "    demo_word = random.choice(test_words)\n",
        "\n",
        "    # Run the demo\n",
        "    play_game_demo(agent, demo_word)\n",
        "\n",
        "    # You can also test a specific word:\n",
        "    # play_game_demo(agent, \"python\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'test_words' or 'agent' not found in memory.\")\n",
        "    print(\"Please make sure you have run all the previous cells in the notebook.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
